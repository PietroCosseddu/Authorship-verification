{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":3381,"datasetId":1968,"databundleVersionId":3381},{"sourceType":"datasetVersion","sourceId":8414791,"datasetId":4887069,"databundleVersionId":8550884},{"sourceType":"datasetVersion","sourceId":8225866,"datasetId":4877399,"databundleVersionId":8351910},{"sourceType":"datasetVersion","sourceId":116883,"datasetId":60268,"databundleVersionId":127425}],"dockerImageVersionId":23026,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notebook description\n\nIn this notebook I've implemented an Authorship verification task using a the ononimus notebook pushlished on Kaggle in 2019. In addition to that implementation I've tried the proposed method on different datasets and also on dataset that wasn't in the training set. Moreover I've also added more metrics (tooken from PAN competitions) to have a better understanding on which are the datasets where the model performs better and why.","metadata":{}},{"cell_type":"markdown","source":"## (To just evaluate the datasets run the code after in the EVALUATION paragraph)","metadata":{}},{"cell_type":"code","source":"#Imports\n\nimport os\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\nimport random\nimport ast  \nimport json\n\n\nfrom time import time\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport datetime\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, LSTM, Lambda\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint\nimport csv\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.metrics import precision_score, recall_score, f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-19T15:18:48.275609Z","iopub.execute_input":"2024-05-19T15:18:48.275844Z","iopub.status.idle":"2024-05-19T15:18:51.050045Z","shell.execute_reply.started":"2024-05-19T15:18:48.275797Z","shell.execute_reply":"2024-05-19T15:18:51.049397Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Implementation with Twitter-data","metadata":{}},{"cell_type":"code","source":"#dataset loading from kaggle's inputs\ndataset = pd.read_excel('/kaggle/input/twitter-data/train.xlsx')\ndataset.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index=range(0,1)\ndf=pd.DataFrame(index=index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset chosen consists in a balanced dataset containing 26k different tweets of 13 different authors, to be able to use it as a dataset for AV task it is necessary to firstly create different pairs of text some of wich will have the same author and some won't. In this case 200k pairs of tweets will be created and used for training, validation and testing.","metadata":{}},{"cell_type":"code","source":"first = True\nwhile (len(df)<200000):\n    for user in dataset.user.unique():\n        dataset = dataset.sample(frac=1).reset_index(drop=True)\n        df1 = dataset[dataset['user'] == user][0:20].reset_index(drop=True)\n        df2 = dataset[dataset['user'] == user][-10:].reset_index(drop=True)\n        df3 = dataset[dataset['user'] != user][-10:].reset_index(drop=True)\n        df3 = pd.concat([df2,df3],ignore_index=True)\n        if first :\n            df = pd.concat([df1,df3],axis = 1)\n            first = False\n        else:\n            df = pd.concat([df,pd.concat([df1,df3],axis = 1)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns = ['user1','text1','user2','text2']\ndf['is_same'] = 0\ndf.loc[df['user1']==df['user2'],'is_same'] = 1\ndf.loc[df['user1']!=df['user2'],'is_same'] = 0\ndf = df.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this implementation the preprocessing steps are pretty simple, firstly the data is cleaned from special characters, then tokenized and enery word is stored in a dictionary and substituited in the dataset with a correspondig token.","metadata":{}},{"cell_type":"code","source":"def text_to_word_list(tweet):\n    ''' Pre process and convert texts to a list of words '''\n    tweet = str(tweet)\n    tweet = tweet.lower()\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n    tweet = re.sub(r'[_\"\\-;%()|.,+&=*%]', '', tweet)\n    tweet = re.sub(r'\\.', ' . ', tweet)\n    tweet = re.sub(r'\\!', ' !', tweet)\n    tweet = re.sub(r'\\?', ' ?', tweet)\n    tweet = re.sub(r'\\,', ' ,', tweet)\n    tweet = re.sub(r':', ' : ', tweet)\n    tweet = re.sub(r'#', ' # ', tweet)\n    tweet = re.sub(r'@', ' @ ', tweet)\n    tweet = re.sub(r'd .c .', 'd.c.', tweet)\n    tweet = re.sub(r'u .s .', 'd.c.', tweet)\n    tweet = re.sub(r' amp ', ' and ', tweet)\n    tweet = re.sub(r'pm', ' pm ', tweet)\n    tweet = re.sub(r'news', ' news ', tweet)\n    tweet = re.sub(r' . . . ', ' ', tweet)\n    tweet = re.sub(r' .  .  . ', ' ', tweet)\n    tweet = re.sub(r' ! ! ', ' ! ', tweet)\n    tweet = re.sub(r'&amp', 'and', tweet)\n    tweet = tweet.split()\n\n    return tweet\n\nvocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\ntweets_cols = ['text1', 'text2']\ndef preprocess(df):\n    for index, row in df.iterrows():\n        for tweet in tweets_cols:\n                integers = []\n                for word in text_to_word_list(row[tweet]):\n                    if word not in vocabulary:\n                        vocabulary[word] = len(inverse_vocabulary)\n                        integers.append(len(inverse_vocabulary))\n                        inverse_vocabulary.append(word)\n                    else:\n                        integers.append(vocabulary[word])\n\n                df.set_value(index, tweet, integers)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=preprocess(df)\n#after the preprocessing steps the max length is computed in order to then pad every text to the max length and have \n    #an omogeneus length in the samples.\nmax_length = max(df.text1.map(lambda x: len(x)).max(),\n                     df.text2.map(lambda x: len(x)).max())\nmax_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.sample(frac=1).reset_index(drop=True) #the data is shuffled before being splitted in train,validation and test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#In the splitting process the data is divided first in 80% train and 20% test and then 10% of the 80% of the training is reserved for validation\nX_train_tw, X_test_tw, Y_train_tw, Y_test_tw = train_test_split(df[tweets_cols], df['is_same'], test_size=0.2)\nX_train_tw, X_validation_tw, Y_train_tw, Y_validation_tw = train_test_split(X_train_tw,Y_train_tw, test_size=0.1)\n\ntest_tw = df[df.index.isin(X_test_tw.index)]\n\nX_train_tw = {'left': X_train_tw.text1, 'right': X_train_tw.text2}\nX_validation_tw = {'left': X_validation_tw.text1, 'right': X_validation_tw.text2}\nX_test_tw = {'left': X_test_tw.text1, 'right': X_test_tw.text2}\n\nY_train_tw = Y_train_tw.values\nY_validation_tw = Y_validation_tw.values\nY_test_tw = Y_test_tw.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Padding all the train, test and validation set to the max length in the case of twitter data the max length of a tweet \n    # is 60 tokens\n    \ndef making_pad(data):\n    for key,val in data.items():\n        data[key]=pad_sequences(data[key],maxlen=maxlength)\n    return data\n\nX_train_tw = making_pad(X_train_tw)\nX_validation_tw = making_pad(X_validation_tw)\nX_test_tw = making_pad(X_test_tw)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_train_tw['left'].shape == X_train_tw['right'].shape\nassert len(X_train_tw['left']) == len(Y_train_tw)\n\nX_train_tw['left'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Definition of the model","metadata":{}},{"cell_type":"code","source":"# Import necessary modules from Keras\nfrom keras import optimizers\nfrom keras.layers import Input, Embedding, LSTM, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\n# Define some constants\nn_hidden = 20  # Number of hidden units in the LSTM layer\nbatch_size = 1024  # Batch size for training\nn_epoch = 10  # Number of epochs for training\n\n# Function to calculate the exponent of the negative Manhattan distance between two vectors\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# Define input layers for left and right sequences\nleft_input = Input(shape=(max_length,), dtype='int32')\nright_input = Input(shape=(max_length,), dtype='int32')\n\n# Create an embedding layer for word embeddings\nembedding_layer = Embedding(len(vocabulary)+1, 100,input_length=max_length)\n\n# Apply the embedding layer to both input sequences\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Define a shared LSTM layer\nshared_lstm = LSTM(n_hidden)\n\n# Apply the shared LSTM to both encoded sequences\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n# Calculate the distance between the outputs of the LSTM layers\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n                         output_shape=lambda x: (x[0][0],1))([left_output, right_output])\n\n# Create a Dense layer with sigmoid activation for prediction\nprediction = Dense(1, activation='sigmoid')(malstm_distance)\n\n# Define the model with inputs and outputs\nmodel = Model([left_input, right_input], outputs=prediction)\n\n# Compile the model with binary crossentropy loss and Adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print a summary of the model architecture\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training\ntraining_start_time = time()\n\nhistory=model.fit([X_train_tw['left'], X_train_tw['right']], Y_train_tw, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation_tw['left'], X_validation_tw['right']], Y_validation_tw))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first impression and possible evaluation of the model after training without counting the new metrics used for the complete evaluation \npreds = model.predict([X_test_tw['left'], X_test_tw['right']],batch_size=256)\n\npreds[preds>=0.5]=1\npreds[preds<0.5]=0\n\nprint(classification_report(Y_test_tw, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saving process of the trained model, and the test set in order to later evaluate the model","metadata":{}},{"cell_type":"code","source":"model.save('modello_addestrato_su_tw')\n\nmy_dict = X_test_tw\n\nserializable_dict = {}\nfor key, value in my_dict.items():\n    if isinstance(value, np.ndarray):\n        serializable_dict[key] = value.tolist()  # Convert NumPy array to list\n    else:\n        serializable_dict[key] = value\n\n# Save dictionary as JSON\nwith open('X_test_TW.json', 'w') as f:\n    json.dump(serializable_dict, f)\n    \nnp.save('Y_test_tw.npy', Y_test_tw)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementation with blog-authorship-corpus\nIn this case the dataset isn't more the one used in the original implementation but there had to be some small changes while working with the data. The data originally contained other labels other than the text and the it that contained information on the user that wrote the single blog post. For this method I removed them and was left with just id and text for every one of the more than 600k rows","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/blog-authorship-corpus/blogtext.csv')\ndataset = dataset.drop(columns=[col for col in dataset.columns if col not in ['id', 'text']])\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(dataset['id'].unique()))\nindex=range(0,1)\ndf=pd.DataFrame(index=index)","metadata":{"_uuid":"ea1c8bc1699b1fdb3a5bd6ff2a46f9967dc77fc1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also in this case I had to manually create the pairs needed to work in an AV task, so the code under does exactly this creating 1 million different pairs balanced with 500k pairs where the author is different and another 500k where the athor is the same","metadata":{}},{"cell_type":"code","source":"def create_balanced_id_pairs(df, num_pairs=1000000):\n    # Check if the DataFrame has enough data to form at least one pair\n    if len(df) < 2:\n        raise ValueError(\"The DataFrame must contain at least two rows to form a pair.\")\n\n    # Separate DataFrame by unique ids and filter out ids with less than two entries\n    grouped = df.groupby('id').filter(lambda x: len(x) > 1).groupby('id')\n\n    # List for storing temporary results\n    pairs = []\n\n    # Half the pairs with the same id\n    same_id_pairs = num_pairs // 2\n    different_id_pairs = num_pairs - same_id_pairs\n\n    # Create pairs with the same id\n    for name, group in grouped:\n        if len(group) >= 2 and same_id_pairs > 0:\n            for _ in range(min(same_id_pairs, len(group) // 2)):  # Limit to available pairs in group\n                sampled_rows = group.sample(n=2)\n                pairs.append({\n                    'id1': sampled_rows.iloc[0]['id'],\n                    'text1': sampled_rows.iloc[0]['text'],\n                    'id2': sampled_rows.iloc[1]['id'],\n                    'text2': sampled_rows.iloc[1]['text']\n                })\n                same_id_pairs -= 1\n\n    # Create pairs with different ids\n    all_ids = list(grouped.groups.keys())\n    while different_id_pairs > 0 and len(all_ids) > 1:\n        selected_ids = random.sample(all_ids, 2)\n        group1, group2 = grouped.get_group(selected_ids[0]), grouped.get_group(selected_ids[1])\n        row1, row2 = group1.sample(n=1).iloc[0], group2.sample(n=1).iloc[0]\n        pairs.append({\n            'id1': row1['id'],\n            'text1': row1['text'],\n            'id2': row2['id'],\n            'text2': row2['text']\n        })\n        different_id_pairs -= 1\n        \n\n    # Create a new DataFrame from the pairs\n    new_df = pd.DataFrame(pairs)\n    \n    return new_df\n\ndf = create_balanced_id_pairs(dataset)\nprint(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['id1','text1','id2','text2']]\ndf['is_same'] = 0\ndf.loc[df['id1']==df['id2'],'is_same'] = 1\ndf.loc[df['id1']!=df['id2'],'is_same'] = 0\ndf = df.sample(frac=1).reset_index(drop=True)","metadata":{"_uuid":"1f5bbee729ff3b1cfbd24602d471cd7beded570d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to work more easily and computationally be able to train a new model with this method from scrath from the original 1 million pairs I removed all the ones that excedeed a certain arbitrary length of (in this case) 750. Doing this I'was left with a little more than 200k pairs, similar number as the number of different pairs used to train and test the model with the original implementation.","metadata":{}},{"cell_type":"code","source":"indices_to_keep = []\n\n# Iterate over each row using the index\nfor i in df.index:\n    if len(df['text1'].loc[i]) <= 750:\n        if len(df['text2'].loc[i]) <= 750:\n            indices_to_keep.append(i)\n\n# Use the list of indices to keep to filter the DataFrame\nfiltered_df = df.loc[indices_to_keep]\nfiltered.to_csv('filtered_df.csv', index=False) \n#The dataset is then saved to be reused later and more easily in a second moment when It'll be time to preprocess the data.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_word_list(tweet):\n    ''' Pre process and convert texts to a list of words '''\n    tweet = str(tweet)\n    tweet = tweet.lower()\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n    tweet = re.sub(r'[_\"\\-;%()|.,+&=*%]', '', tweet)\n    tweet = re.sub(r'\\.', ' . ', tweet)\n    tweet = re.sub(r'\\!', ' !', tweet)\n    tweet = re.sub(r'\\?', ' ?', tweet)\n    tweet = re.sub(r'\\,', ' ,', tweet)\n    tweet = re.sub(r':', ' : ', tweet)\n    tweet = re.sub(r'#', ' # ', tweet)\n    tweet = re.sub(r'@', ' @ ', tweet)\n    tweet = re.sub(r'd .c .', 'd.c.', tweet)\n    tweet = re.sub(r'u .s .', 'd.c.', tweet)\n    tweet = re.sub(r' amp ', ' and ', tweet)\n    tweet = re.sub(r'pm', ' pm ', tweet)\n    tweet = re.sub(r'news', ' news ', tweet)\n    tweet = re.sub(r' . . . ', ' ', tweet)\n    tweet = re.sub(r' .  .  . ', ' ', tweet)\n    tweet = re.sub(r' ! ! ', ' ! ', tweet)\n    tweet = re.sub(r'&amp', 'and', tweet)\n    tweet = tweet.split()\n\n    return tweet\n\nvocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\ntweets_cols = ['text1', 'text2']\ndef preprocess(df):\n    for index, row in df.iterrows():\n        for tweet in tweets_cols:\n                integers = []\n                for word in text_to_word_list(row[tweet]):\n                    if word not in vocabulary:\n                        vocabulary[word] = len(inverse_vocabulary)\n                        integers.append(len(inverse_vocabulary))\n                        inverse_vocabulary.append(word)\n                    else:\n                        integers.append(vocabulary[word])\n\n                df.set_value(index, tweet, integers)\n    return df","metadata":{"_uuid":"544a34712a3421d50c249e6a2499807da17d53cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the prevoiusly prepared and filtered dataset ready to be preprocessed\nfiltered_df = pd.read_csv('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/filtered_df.csv')\nfiltered_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preprocessed = preprocess(filtered_df) #the preprocessing steps are exactly the same as the original implementation,\n#cleaning of the text, tokenization of the word and saving the words in a dictionary\nmax_length = max(df_preprocessed.text1.map(lambda x: len(x)).max(),\n                     df_preprocessed.text2.map(lambda x: len(x)).max())\nprint(len(df_preprocessed), len(vocabulary),max_length) #In this case the max length will be higher than the implementation usgin the twitter data but not to much higher \n#and we'll be able to train another model on this dataset from scratch using the same architecture","metadata":{"_uuid":"641d0f5fdaff2f2362b3b3824658184bcf713d3b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffling and data splitting in train, validation and test, same as the original implementation above\ndf_preprocessed = df_preprocessed.sample(frac=1).reset_index(drop=True)\n\nX_train, X_test, Y_train, Y_test = train_test_split(df_preprocessed[tweets_cols], df_preprocessed['is_same'], test_size=0.2)\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train,Y_train, test_size=0.1)\n\ntest = df_preprocessed[df_preprocessed.index.isin(X_test.index)]\n\nX_train = {'left': X_train.text1, 'right': X_train.text2}\nX_validation = {'left': X_validation.text1, 'right': X_validation.text2}\nX_test = {'left': X_test.text1, 'right': X_test.text2}\n\nY_train = Y_train.values\nY_validation = Y_validation.values\nY_test = Y_test.values","metadata":{"_uuid":"9a5528c3dff81735b6cbacb151842fe549bcda95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#padding to bring all the text to the same length, with the filtered dataet the max length will be 196 tokens for a single blog post\ndef making_pad(data):\n    for key,val in data.items():\n        data[key]=pad_sequences(data[key],maxlen=max_length)\n    return data\n\nX_train = making_pad(X_train)\nX_validation = making_pad(X_validation)\nX_test = making_pad(X_test)","metadata":{"_uuid":"eebd90a75dab7316b130d23697bc7f94c62a81d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_train['left'].shape == X_train['right'].shape\nassert len(X_train['left']) == len(Y_train)\n\nX_train['left'].shape","metadata":{"_uuid":"49b62cbde7feb6bdaa3757ff216532d1546bff19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules from Keras\nfrom keras import optimizers\nfrom keras.layers import Input, Embedding, LSTM, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\n# Define some constants\nn_hidden = 20  # Number of hidden units in the LSTM layer\nbatch_size = 1024  # Batch size for training\nn_epoch = 10  # Number of epochs for training\n\n# Function to calculate the exponent of the negative Manhattan distance between two vectors\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# Define input layers for left and right sequences\nleft_input = Input(shape=(max_length,), dtype='int32')\nright_input = Input(shape=(max_length,), dtype='int32')\n\n# Create an embedding layer for word embeddings\nembedding_layer = Embedding(len(vocabulary)+1, 100,input_length=max_length)\n\n# Apply the embedding layer to both input sequences\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Define a shared LSTM layer\nshared_lstm = LSTM(n_hidden)\n\n# Apply the shared LSTM to both encoded sequences\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n# Calculate the distance between the outputs of the LSTM layers\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n                         output_shape=lambda x: (x[0][0],1))([left_output, right_output])\n\n# Create a Dense layer with sigmoid activation for prediction\nprediction = Dense(1, activation='sigmoid')(malstm_distance)\n\n# Define the model with inputs and outputs\nmodel = Model([left_input, right_input], outputs=prediction)\n\n# Compile the model with binary crossentropy loss and Adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print a summary of the model architecture\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training\ntraining_start_time = time()\n\nhistory=model.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))","metadata":{"_uuid":"becfa8df319b7654b05a5a3b7a59ff0f1b2675d2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#as in the previous case the model and the test set is saved in order to easilly evaluate later the method with this dataset\nmodel.save('modello_addestrato_su_BAC')\n\nmy_dict = X_test\n\nserializable_dict = {}\nfor key, value in my_dict.items():\n    if isinstance(value, np.ndarray):\n        serializable_dict[key] = value.tolist()  # Convert NumPy array to list\n    else:\n        serializable_dict[key] = value\n\n# Save dictionary as JSON\nwith open('X_test_BAC.json', 'w') as f:\n    json.dump(serializable_dict, f)\n\nnp.save('Y_test.npy', Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict([X_test['left'], X_test['right']],batch_size=256)\n\npreds[preds>=0.5]=1\npreds[preds<0.5]=0\n\nprint(classification_report(Y_test, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEST WITH PAN","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_json('/kaggle/input/pan20-av-train/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small.jsonl', lines = True)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lunghezze = []\nfor i in range(len(dataset)):\n    lunghezze.append(len(dataset['pair'].loc[i][0]))\n    lunghezze.append(len(dataset['pair'].loc[i][1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(lunghezze)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_list(row):\n    return row[0], row[1]\n\n# Applica la funzione alla colonna combined_list e assegna i risultati a due nuove colonne\ndataset[['text1', 'text2']] = dataset['pair'].apply(lambda x: pd.Series(split_list(x)))\n\n# Rimuovi la colonna combined_list se necessario\ndataset = dataset.drop(columns=['pair', 'fandoms'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"truth = pd.read_json('/kaggle/input/pan20-av-train/pan20-authorship-verification-training-small/pan20-authorship-verification-training-small-truth.jsonl', lines = True)\ntruth.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['truth'] = truth['same'] \ndataset['truth'] = dataset['truth'].astype(int)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_word_list(tweet):\n    ''' Pre process and convert texts to a list of words '''\n    tweet = str(tweet)\n    tweet = tweet.lower()\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n    tweet = re.sub(r'[_\"\\-;%()|.,+&=*%]', '', tweet)\n    tweet = re.sub(r'\\.', ' . ', tweet)\n    tweet = re.sub(r'\\!', ' !', tweet)\n    tweet = re.sub(r'\\?', ' ?', tweet)\n    tweet = re.sub(r'\\,', ' ,', tweet)\n    tweet = re.sub(r':', ' : ', tweet)\n    tweet = re.sub(r'#', ' # ', tweet)\n    tweet = re.sub(r'@', ' @ ', tweet)\n    tweet = re.sub(r'd .c .', 'd.c.', tweet)\n    tweet = re.sub(r'u .s .', 'd.c.', tweet)\n    tweet = re.sub(r' amp ', ' and ', tweet)\n    tweet = re.sub(r'pm', ' pm ', tweet)\n    tweet = re.sub(r'news', ' news ', tweet)\n    tweet = re.sub(r' . . . ', ' ', tweet)\n    tweet = re.sub(r' .  .  . ', ' ', tweet)\n    tweet = re.sub(r' ! ! ', ' ! ', tweet)\n    tweet = re.sub(r'&amp', 'and', tweet)\n    tweet = tweet.split()\n\n    return tweet\n\nvocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\ntweets_cols = ['text1', 'text2']\ndef preprocess(df):\n    for index, row in df.iterrows():\n        for tweet in tweets_cols:\n                integers = []\n                for word in text_to_word_list(row[tweet]):\n                    if word not in vocabulary:\n                        vocabulary[word] = len(inverse_vocabulary)\n                        integers.append(len(inverse_vocabulary))\n                        inverse_vocabulary.append(word)\n                    else:\n                        integers.append(vocabulary[word])\n\n                df.set_value(index, tweet, integers)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAN_preprocessed = preprocess(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAN_preprocessed.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(PAN_preprocessed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices_to_keep = []\n\n# Iterate over each row using the index\nfor i in PAN_preprocessed.index:\n    if len(PAN_preprocessed['text1'].loc[i]) <= 25000:\n        if len(PAN_preprocessed['text2'].loc[i]) <= 25000:\n            indices_to_keep.append(i)\n\n# Use the list of indices to keep to filter the DataFrame\nPAN_preprocessed = PAN_preprocessed.loc[indices_to_keep]\nlen(PAN_preprocessed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAN_preprocessed = PAN_preprocessed.sample(frac=1).reset_index(drop=True)\n\nX_train_PAN, X_test_PAN, Y_train_PAN, Y_test_PAN = train_test_split(PAN_preprocessed[tweets_cols], PAN_preprocessed['truth'], test_size=0.2)\nX_train_PAN, X_validation_PAN, Y_train_PAN, Y_validation_PAN = train_test_split(X_train_PAN,Y_train_PAN, test_size=0.1)\n\ntest = PAN_preprocessed[PAN_preprocessed.index.isin(X_test_PAN.index)]\n\nX_train_PAN = {'left': X_train_PAN.text1, 'right': X_train_PAN.text2}\nX_validation_PAN = {'left': X_validation_PAN.text1, 'right': X_validation_PAN.text2}\nX_test_PAN = {'left': X_test_PAN.text1, 'right': X_test_PAN.text2}\n\nY_train_PAN = Y_train_PAN.values\nY_validation_PAN = Y_validation_PAN.values\nY_test_PAN = Y_test_PAN.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = max(PAN_preprocessed.text1.map(lambda x: len(x)).max(),\n                     PAN_preprocessed.text2.map(lambda x: len(x)).max())\nmax_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def making_pad(data):\n    for key,val in data.items():\n        data[key]=pad_sequences(data[key],maxlen=max_length)\n    return data\n\nX_train_PAN = making_pad(X_train_PAN)\nX_validation_PAN = making_pad(X_validation_PAN)\nX_test_PAN = making_pad(X_test_PAN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_train_PAN['left'].shape == X_train_PAN['right'].shape\nassert len(X_train_PAN['left']) == len(Y_train_PAN)\n\nX_train_PAN['left'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_dict = X_test_PAN\n\nserializable_dict = {}\nfor key, value in my_dict.items():\n    if isinstance(value, np.ndarray):\n        serializable_dict[key] = value.tolist()  # Convert NumPy array to list\n    else:\n        serializable_dict[key] = value\n\n# Save dictionary as JSON\nwith open('X_test_PAN.json', 'w') as f:\n    json.dump(serializable_dict, f)\n    \nnp.save('Y_test_PAN.npy', Y_test_PAN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the extremely large number of tokens in each text in this dataset, training a model from scratch on this data proved to be impossible. In order to address this challenge, I trained another model named \"modello_addestrato_su_BAC_senza_limiti_di_lunghezza\". This model essentially replicates the same architecture as the original model but was trained on a filtered version of the Blog Authorship Corpus dataset. Importantly, in this model, there is no predetermined limit on the length of the tokens provided as input, enabling it to handle longer text pairs.\n\nThe code provided below, which involves creating and training the model with the PAN training set, serves as an example and illustration of the process. However, it's worth noting that the free GPU P100 available on Kaggle Notebooks was insufficient for training a model of this scale.","metadata":{}},{"cell_type":"code","source":"from keras import optimizers\nfrom keras.layers import merge ,Dense,Concatenate\nn_hidden = 20\nbatch_size = 32\nn_epoch = 10\nmax_length = None\n\n\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n\nleft_input = Input(shape=(max_length,), dtype='int32')\nright_input = Input(shape=(max_length,), dtype='int32')\n\nembedding_layer = Embedding((len(vocabulary)//2)+1, 100 ,input_length=max_length)\n\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\nshared_lstm = LSTM(n_hidden)\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n# Calculates the distance as defined by the MaLSTM model\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0],1))([left_output,right_output])\nprediction = Dense(1,activation='sigmoid')(malstm_distance)\n# Pack it all up into a model\nmodel = Model([left_input, right_input],outputs=prediction)\n\n# Adadelta optimizer, with gradient clipping by norm\n# optimizer = optimizers.Adam(clipnorm=gradient_clipping_norm)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training\ntraining_start_time = time()\n\nhistory=model.fit([X_train_PAN['left'], X_train_PAN['right']], Y_train_PAN, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation_PAN['left'], X_validation_PAN['right']], Y_validation_PAN))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEST WITH PAN22","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_json('/kaggle/input/data-and-modelli/pan22-authorship-verification-training.jsonl', lines = True)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lunghezze = []\nfor i in range(len(dataset)):\n    lunghezze.append(len(dataset['pair'].loc[i][0]))\n    lunghezze.append(len(dataset['pair'].loc[i][1]))\nnp.mean(lunghezze)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_list(row):\n    return row[0], row[1]\n\n# Applica la funzione alla colonna combined_list e assegna i risultati a due nuove colonne\ndataset[['text1', 'text2']] = dataset['pair'].apply(lambda x: pd.Series(split_list(x)))\n\n# Rimuovi la colonna combined_list se necessario\ndataset = dataset.drop(columns=['pair', 'discourse_types'])\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"truth = pd.read_json('/kaggle/input/data-and-modelli/pan22-authorship-verification-training-truth.jsonl', lines = True)\ndataset['truth'] = truth['same'] \ndataset['truth'] = dataset['truth'].astype(int)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_word_list(tweet):\n    ''' Pre process and convert texts to a list of words '''\n    tweet = str(tweet)\n    tweet = tweet.lower()\n    tweet = tweet.lower()\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n    tweet = re.sub(r'[_\"\\-;%()|.,+&=*%]', '', tweet)\n    tweet = re.sub(r'\\.', ' . ', tweet)\n    tweet = re.sub(r'\\!', ' !', tweet)\n    tweet = re.sub(r'\\?', ' ?', tweet)\n    tweet = re.sub(r'\\,', ' ,', tweet)\n    tweet = re.sub(r':', ' : ', tweet)\n    tweet = re.sub(r'#', ' # ', tweet)\n    tweet = re.sub(r'@', ' @ ', tweet)\n    tweet = re.sub(r'd .c .', 'd.c.', tweet)\n    tweet = re.sub(r'u .s .', 'd.c.', tweet)\n    tweet = re.sub(r' amp ', ' and ', tweet)\n    tweet = re.sub(r'pm', ' pm ', tweet)\n    tweet = re.sub(r'news', ' news ', tweet)\n    tweet = re.sub(r' . . . ', ' ', tweet)\n    tweet = re.sub(r' .  .  . ', ' ', tweet)\n    tweet = re.sub(r' ! ! ', ' ! ', tweet)\n    tweet = re.sub(r'&amp', 'and', tweet)\n    tweet = tweet.split()\n\n    return tweet\n\nvocabulary = dict()\ninverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\ntweets_cols = ['text1', 'text2']\ndef preprocess(df):\n    for index, row in df.iterrows():\n        for tweet in tweets_cols:\n                integers = []\n                for word in text_to_word_list(row[tweet]):\n                    if word not in vocabulary:\n                        vocabulary[word] = len(inverse_vocabulary)\n                        integers.append(len(inverse_vocabulary))\n                        inverse_vocabulary.append(word)\n                    else:\n                        integers.append(vocabulary[word])\n\n                df.set_value(index, tweet, integers)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAN22_preprocessed = preprocess(dataset)\nPAN22_preprocessed.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = max(PAN22_preprocessed.text1.map(lambda x: len(x)).max(),\n                     PAN22_preprocessed.text2.map(lambda x: len(x)).max())\nmax_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PAN22_preprocessed = PAN22_preprocessed.sample(frac=1).reset_index(drop=True)\n\nX_train_PAN22, X_test_PAN22, Y_train_PAN22, Y_test_PAN22 = train_test_split(PAN22_preprocessed[tweets_cols], PAN22_preprocessed['truth'], test_size=0.2)\nX_train_PAN22, X_validation_PAN22, Y_train_PAN22, Y_validation_PAN22 = train_test_split(X_train_PAN22,Y_train_PAN22, test_size=0.1)\n\ntest = PAN22_preprocessed[PAN22_preprocessed.index.isin(X_test_PAN22.index)]\n\nX_train_PAN22 = {'left': X_train_PAN22.text1, 'right': X_train_PAN22.text2}\nX_validation_PAN22 = {'left': X_validation_PAN22.text1, 'right': X_validation_PAN22.text2}\nX_test_PAN22 = {'left': X_test_PAN22.text1, 'right': X_test_PAN22.text2}\n\nY_train_PAN22 = Y_train_PAN22.values\nY_validation_PAN22 = Y_validation_PAN22.values\nY_test_PAN22 = Y_test_PAN22.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def making_pad(data):\n    for key,val in data.items():\n        data[key]=pad_sequences(data[key],maxlen=max_length)\n    return data\n\nX_train_PAN22 = making_pad(X_train_PAN22)\nX_validation_PAN22 = making_pad(X_validation_PAN22)\nX_test_PAN22 = making_pad(X_test_PAN22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert X_train_PAN22['left'].shape == X_train_PAN22['right'].shape\nassert len(X_train_PAN22['left']) == len(Y_train_PAN22)\n\nX_train_PAN22['left'].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules from Keras\nfrom keras import optimizers\nfrom keras.layers import Input, Embedding, LSTM, Lambda, Dense\nfrom keras.models import Model\nimport keras.backend as K\n\n# Define some constants\nn_hidden = 20  # Number of hidden units in the LSTM layer\nbatch_size = 256  # Batch size for training\nn_epoch = 10  # Number of epochs for training\n\n# Function to calculate the exponent of the negative Manhattan distance between two vectors\ndef exponent_neg_manhattan_distance(left, right):\n    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# Define input layers for left and right sequences\nleft_input = Input(shape=(max_length,), dtype='int32')\nright_input = Input(shape=(max_length,), dtype='int32')\n\n# Create an embedding layer for word embeddings\nembedding_layer = Embedding(len(vocabulary)+1, 100,input_length=max_length)\n\n# Apply the embedding layer to both input sequences\nencoded_left = embedding_layer(left_input)\nencoded_right = embedding_layer(right_input)\n\n# Define a shared LSTM layer\nshared_lstm = LSTM(n_hidden)\n\n# Apply the shared LSTM to both encoded sequences\nleft_output = shared_lstm(encoded_left)\nright_output = shared_lstm(encoded_right)\n\n# Calculate the distance between the outputs of the LSTM layers\nmalstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n                         output_shape=lambda x: (x[0][0],1))([left_output, right_output])\n\n# Create a Dense layer with sigmoid activation for prediction\nprediction = Dense(1, activation='sigmoid')(malstm_distance)\n\n# Define the model with inputs and outputs\nmodel = Model([left_input, right_input], outputs=prediction)\n\n# Compile the model with binary crossentropy loss and Adam optimizer\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Print a summary of the model architecture\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training\ntraining_start_time = time()\n\nhistory=model.fit([X_train_PAN22['left'], X_train_PAN22['right']], Y_train_PAN22, batch_size=batch_size, nb_epoch=n_epoch,\n                            validation_data=([X_validation_PAN22['left'], X_validation_PAN22['right']], Y_validation_PAN22))\n\nprint(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('modello_addestrato_su_PAN22')\n\nmy_dict = X_test_PAN22\n\nserializable_dict = {}\nfor key, value in my_dict.items():\n    if isinstance(value, np.ndarray):\n        serializable_dict[key] = value.tolist()  # Convert NumPy array to list\n    else:\n        serializable_dict[key] = value\n\n# Save dictionary as JSON\nwith open('X_test_PAN22.json', 'w') as f:\n    json.dump(serializable_dict, f)\n    \nnp.save('Y_test_PAN22.npy', Y_test_PAN22)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EVALUATION\n\nThe evaluation metrics for these models were selected based on previous PAN Author Verification competitions and commonly used metrics such as precision and recall. The code to calculate C@1 and AUC was directly taken from the PAN competition's official repository. Some metrics were employed to reward models that produced a high number of uncertain responses rather than simply favoring a higher number of correct responses. To calculate these metrics, a custom threshold was utilized to determine when a prediction was uncertain (typically between 0.45 and 0.55), and such predictions were labeled as abstained predictions by the model.","metadata":{}},{"cell_type":"code","source":"def c_at_1(true_y, pred_y, threshold=5):\n    \"\"\"\n    Calculates the c@1 score, an evaluation method specific to the\n    PAN competition. This method rewards predictions which leave\n    some problems unanswered (score = 0.5). See:\n\n        A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n        In Proc. of the 49th Annual Meeting of the Association for\n        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n\n    Parameters\n    ----------\n    prediction_scores : array [n_problems]\n\n        The predictions outputted by a verification system.\n        Assumes `0 >= prediction <=1`.\n\n    ground_truth_scores : array [n_problems]\n\n        The gold annotations provided for each problem.\n        Will always be `0` or `1`.\n\n    Returns\n    ----------\n    c@1 = the c@1 measure (which accounts for unanswered\n        problems.)\n\n\n    References\n    ----------\n        - E. Stamatatos, et al. Overview of the Author Identification\n        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n        - A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n        In Proc. of the 49th Annual Meeting of the Association for\n        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n\n    \"\"\"\n\n    n = float(len(pred_y))\n    nc, nu = 0.0, 0.0\n\n    for gt_score, pred_score in zip(true_y, pred_y):\n        if pred_score == threshold:\n            nu += 1\n        elif (pred_score > 0.5) == (gt_score > 0.5):\n            nc += 1.0\n    \n    return (1 / n) * (nc + (nu * nc / n))\n\n\ndef auc(true_y, pred_y):\n    \"\"\"\n    Calculates the AUC score (Area Under the Curve), a well-known\n    scalar evaluation score for binary classifiers. This score\n    also considers \"unanswered\" problem, where score = 0.5.\n\n    Parameters\n    ----------\n    prediction_scores : array [n_problems]\n\n        The predictions outputted by a verification system.\n        Assumes `0 >= prediction <=1`.\n\n    ground_truth_scores : array [n_problems]\n\n        The gold annotations provided for each problem.\n        Will typically be `0` or `1`.\n\n    Returns\n    ----------\n    auc = the Area Under the Curve.\n\n    References\n    ----------\n        E. Stamatatos, et al. Overview of the Author Identification\n        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n\n    \"\"\"\n    try:\n        return roc_auc_score(true_y, pred_y)\n    except ValueError:\n        return 0.0","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:19:05.805743Z","iopub.execute_input":"2024-05-19T15:19:05.806012Z","iopub.status.idle":"2024-05-19T15:19:05.821162Z","shell.execute_reply.started":"2024-05-19T15:19:05.805972Z","shell.execute_reply":"2024-05-19T15:19:05.820355Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#loading back all the models and all the test set for each dataset\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import load_model\n\n# Define the exponent_neg_manhattan_distance function\ndef exponent_neg_manhattan_distance(left, right):\n    return K.exp(-K.sum(K.abs(left - right), axis=1, keepdims=True))\n\n# Register custom function with Keras\nkeras.utils.get_custom_objects()['exponent_neg_manhattan_distance'] = exponent_neg_manhattan_distance\n\n# Clear TensorFlow session\nK.clear_session()\n\n# Load the model\nmodel_BAC = load_model('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/modello_addestrato_su_BAC (1)')\nmodel_TW = load_model('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/modello_addestrato_su_tw')\nmodel_no_limits_BAC = load_model('/kaggle/input/data-and-modelli/Modello_addestrato_su_BAC_senza_limit_lunghezza')\nmodel_trained_on_PAN22 = load_model('/kaggle/input/data-and-modelli/modello_addestrato_su_PAN22')\n\nwith open('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/X_test_BAC.json', 'r') as f:\n    test_BAC = json.load(f)\nY_test_BAC = np.load('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/Y_test.npy')\n\nwith open('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/X_test_TW.json', 'r') as f:\n    test_TW = json.load(f)\nY_test_TW = np.load('/kaggle/input/data-and-modelli/DATI PROGETTO AAA/Y_test_tw.npy')\n\nwith open('/kaggle/input/data-and-modelli/X_test_PAN.json', 'r') as f:\n    test_PAN = json.load(f)\nY_test_PAN = np.load('/kaggle/input/data-and-modelli/Y_test_PAN.npy')\n\nwith open('/kaggle/input/data-and-modelli/X_test_PAN22.json', 'r') as f:\n    test_PAN22 = json.load(f)\nY_test_PAN22 = np.load('/kaggle/input/data-and-modelli/Y_test_PAN22.npy')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:19:08.386196Z","iopub.execute_input":"2024-05-19T15:19:08.386530Z","iopub.status.idle":"2024-05-19T15:21:27.539121Z","shell.execute_reply.started":"2024-05-19T15:19:08.386481Z","shell.execute_reply":"2024-05-19T15:21:27.538224Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:107: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 258648500 elements. This may consume a large amount of memory.\n  num_elements)\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:107: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 258648500 elements. This may consume a large amount of memory.\n  num_elements)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For the blog authorship corpus there are two different evaluation, one using the model trained on fixed length for the input and one with the model also used to evaluate the PAN test set trained without a predefined length for the input","metadata":{}},{"cell_type":"code","source":"preds_BAC = model_BAC.predict([test_BAC['left'], test_BAC['right']],batch_size=256)\n\npreds_BAC_probabilities = preds_BAC\n\npreds_BAC_no_astenuti = preds_BAC\npreds_BAC_no_astenuti[preds_BAC>=0.5]=1\npreds_BAC_no_astenuti[preds_BAC<0.5]=0\n\npreds_BAC[preds_BAC>=0.55]=1\npreds_BAC[preds_BAC<0.45]=0\npreds_BAC[(preds_BAC >= 0.45) & (preds_BAC < 0.55)] = 5\n\n#********************************************************************************************************************************************************************************\n\nprecision, recall, f_beta, _ = precision_recall_fscore_support(Y_test_BAC, preds_BAC_no_astenuti, beta=0.5, average=\"binary\")\nbrier_score = 1-brier_score_loss(Y_test_BAC, preds_BAC_probabilities)\nroc_auc = roc_auc_score(Y_test_BAC, preds_BAC)\nc_at_1_score = c_at_1(Y_test_BAC, preds_BAC)\nprecision = precision_score(Y_test_BAC, preds_BAC_no_astenuti)\nrecall = recall_score(Y_test_BAC, preds_BAC_no_astenuti)\nf1 = f1_score(Y_test_BAC, preds_BAC_no_astenuti)\n\nresults_df = pd.DataFrame({\n    'Metric': ['ROC AUC Score', 'c_at_1', 'F0.5', 'Brier Score', 'Precision', 'Recall', 'f1-score'],\n    'Value': [roc_auc, c_at_1_score, f_beta, brier_score, precision, recall, f1]\n})\n\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:21:27.540472Z","iopub.execute_input":"2024-05-19T15:21:27.540693Z","iopub.status.idle":"2024-05-19T15:21:54.688564Z","shell.execute_reply.started":"2024-05-19T15:21:27.540655Z","shell.execute_reply":"2024-05-19T15:21:54.687665Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"          Metric     Value\n0  ROC AUC Score  0.728207\n1         c_at_1  0.734298\n2           F0.5  0.770077\n3    Brier Score  0.734298\n4      Precision  0.770628\n5         Recall  0.767882\n6       f1-score  0.769253\n","output_type":"stream"}]},{"cell_type":"code","source":"preds_BAC = model_no_limits_BAC.predict([test_BAC['left'], test_BAC['right']],batch_size=256)\n\npreds_BAC_probabilities = preds_BAC\n\npreds_BAC_no_astenuti = preds_BAC\npreds_BAC_no_astenuti[preds_BAC>=0.5]=1\npreds_BAC_no_astenuti[preds_BAC<0.5]=0\n\npreds_BAC[preds_BAC>=0.55]=1\npreds_BAC[preds_BAC<0.45]=0\npreds_BAC[(preds_BAC >= 0.45) & (preds_BAC < 0.55)] = 5\n\n#********************************************************************************************************************************************************************************\n\nprecision, recall, f_beta, _ = precision_recall_fscore_support(Y_test_BAC, preds_BAC_no_astenuti, beta=0.5, average=\"binary\")\nbrier_score = 1-brier_score_loss(Y_test_BAC, preds_BAC_probabilities)\nroc_auc = roc_auc_score(Y_test_BAC, preds_BAC)\nc_at_1_score = c_at_1(Y_test_BAC, preds_BAC)\nprecision = precision_score(Y_test_BAC, preds_BAC_no_astenuti)\nrecall = recall_score(Y_test_BAC, preds_BAC_no_astenuti)\nf1 = f1_score(Y_test_BAC, preds_BAC_no_astenuti)\n\nresults_df = pd.DataFrame({\n    'Metric': ['ROC AUC Score', 'c_at_1', 'F0.5', 'Brier Score', 'Precision', 'Recall', 'f1-score'],\n    'Value': [roc_auc, c_at_1_score, f_beta, brier_score, precision, recall, f1]\n})\n\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:21:54.689911Z","iopub.execute_input":"2024-05-19T15:21:54.690259Z","iopub.status.idle":"2024-05-19T15:22:16.787252Z","shell.execute_reply.started":"2024-05-19T15:21:54.690162Z","shell.execute_reply":"2024-05-19T15:22:16.786532Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"          Metric     Value\n0  ROC AUC Score  0.854158\n1         c_at_1  0.855872\n2           F0.5  0.879012\n3    Brier Score  0.855872\n4      Precision  0.882504\n5         Recall  0.865320\n6       f1-score  0.873827\n","output_type":"stream"}]},{"cell_type":"code","source":"preds_TW = model_TW.predict([test_TW['left'], test_TW['right']],batch_size=256)\n\npreds_TW_probabilities = preds_TW\n\npreds_TW_no_astenuti = preds_TW\npreds_TW_no_astenuti[preds_TW>=0.5]=1\npreds_TW_no_astenuti[preds_TW<0.5]=0\n\npreds_TW[preds_TW>=0.55]=1\npreds_TW[preds_TW<0.45]=0\npreds_TW[(preds_TW >= 0.45) & (preds_TW < 0.55)] = 5\n\n#*******************************************************************************************************************************************************************************************************************************\n\nprecision, recall, f_beta, _ = precision_recall_fscore_support(Y_test_TW, preds_TW_no_astenuti, beta=0.5, average=\"binary\")\nbrier_score = 1-brier_score_loss(Y_test_TW, preds_TW_probabilities)\nroc_auc = roc_auc_score(Y_test_TW, preds_TW)\nc_at_1_score = c_at_1(Y_test_TW, preds_TW)\nprecision = precision_score(Y_test_TW, preds_TW_no_astenuti)\nrecall = recall_score(Y_test_TW, preds_TW_no_astenuti)\nf1 = f1_score(Y_test_TW, preds_TW_no_astenuti)\n\nresults_df = pd.DataFrame({\n    'Metric': ['ROC AUC Score', 'c_at_1', 'F0.5', 'Brier Score', 'Precision', 'Recall', 'f1-score'],\n    'Value': [roc_auc, c_at_1_score, f_beta, brier_score, precision, recall, f1]\n})\n\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:22:16.788652Z","iopub.execute_input":"2024-05-19T15:22:16.788956Z","iopub.status.idle":"2024-05-19T15:22:41.700372Z","shell.execute_reply.started":"2024-05-19T15:22:16.788903Z","shell.execute_reply":"2024-05-19T15:22:41.699399Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"          Metric     Value\n0  ROC AUC Score  0.877343\n1         c_at_1  0.877348\n2           F0.5  0.889363\n3    Brier Score  0.877348\n4      Precision  0.900095\n5         Recall  0.848878\n6       f1-score  0.873737\n","output_type":"stream"}]},{"cell_type":"code","source":"preds_PAN = model_no_limits_BAC.predict([test_PAN['left'], test_PAN['right']],batch_size=256)\n\npreds_PAN_probabilities = preds_PAN\n\npreds_PAN_no_astenuti = preds_PAN\npreds_PAN_no_astenuti[preds_PAN>=0.5]=1\npreds_PAN_no_astenuti[preds_PAN<0.5]=0\n\npreds_PAN[preds_PAN>=0.55]=1\npreds_PAN[preds_PAN<0.45]=0\npreds_PAN[(preds_PAN >= 0.45) & (preds_PAN < 0.55)] = 5\n\n#***********************************************************************************************************************************************************\n\nprecision, recall, f_beta, _ = precision_recall_fscore_support(Y_test_PAN, preds_PAN_no_astenuti, beta=0.5, average=\"binary\")\nbrier_score = 1-brier_score_loss(Y_test_PAN, preds_PAN_probabilities)\nroc_auc = roc_auc_score(Y_test_PAN, preds_PAN)\nc_at_1_score = c_at_1(Y_test_PAN, preds_PAN)\nprecision = precision_score(Y_test_PAN, preds_PAN_no_astenuti)\nrecall = recall_score(Y_test_PAN, preds_PAN_no_astenuti)\nf1 = f1_score(Y_test_PAN, preds_PAN_no_astenuti)\n\nresults_df = pd.DataFrame({\n    'Metric': ['ROC AUC Score', 'c_at_1', 'F0.5', 'Brier Score', 'Precision', 'Recall', 'f1-score'],\n    'Value': [roc_auc, c_at_1_score, f_beta, brier_score, precision, recall, f1]\n})\n\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:22:41.701819Z","iopub.execute_input":"2024-05-19T15:22:41.702130Z","iopub.status.idle":"2024-05-19T15:32:45.304759Z","shell.execute_reply.started":"2024-05-19T15:22:41.702071Z","shell.execute_reply":"2024-05-19T15:32:45.304025Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"          Metric     Value\n0  ROC AUC Score  0.502423\n1         c_at_1  0.481175\n2           F0.5  0.383312\n3    Brier Score  0.481175\n4      Precision  0.539588\n5         Recall  0.177583\n6       f1-score  0.267222\n","output_type":"stream"}]},{"cell_type":"code","source":"preds_PAN22 = model_trained_on_PAN22.predict([test_PAN22['left'], test_PAN22['right']],batch_size=256)\n\npreds_PAN22_probabilities = preds_PAN22\n\npreds_PAN22_no_astenuti = preds_PAN22\npreds_PAN22_no_astenuti[preds_PAN22>=0.5]=1\npreds_PAN22_no_astenuti[preds_PAN22<0.5]=0\n\npreds_PAN22[preds_PAN22>=0.55]=1\npreds_PAN22[preds_PAN22<0.45]=0\npreds_PAN22[(preds_PAN22 >= 0.45) & (preds_PAN22 < 0.55)] = 5\n\n#***********************************************************************************************************************************************************\n\nprecision, recall, f_beta, _ = precision_recall_fscore_support(Y_test_PAN22, preds_PAN22_no_astenuti, beta=0.5, average=\"binary\")\nbrier_score = 1-brier_score_loss(Y_test_PAN22, preds_PAN22_probabilities)\nroc_auc = roc_auc_score(Y_test_PAN22, preds_PAN22)\nc_at_1_score = c_at_1(Y_test_PAN22, preds_PAN22)\nprecision = precision_score(Y_test_PAN22, preds_PAN22_no_astenuti)\nrecall = recall_score(Y_test_PAN22, preds_PAN22_no_astenuti)\nf1 = f1_score(Y_test_PAN22, preds_PAN22_no_astenuti)\n\nresults_df = pd.DataFrame({\n    'Metric': ['ROC AUC Score', 'c_at_1', 'F0.5', 'Brier Score', 'Precision', 'Recall', 'f1-score'],\n    'Value': [roc_auc, c_at_1_score, f_beta, brier_score, precision, recall, f1]\n})\n\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:32:45.306085Z","iopub.execute_input":"2024-05-19T15:32:45.306371Z","iopub.status.idle":"2024-05-19T15:33:11.507663Z","shell.execute_reply.started":"2024-05-19T15:32:45.306314Z","shell.execute_reply":"2024-05-19T15:33:11.506818Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"          Metric     Value\n0  ROC AUC Score  0.756057\n1         c_at_1  0.753771\n2           F0.5  0.779221\n3    Brier Score  0.753771\n4      Precision  0.802893\n5         Recall  0.697017\n6       f1-score  0.746218\n","output_type":"stream"}]}]}